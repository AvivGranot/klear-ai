/**
 * AI module with multiple LLM backends
 * Priority: 1) Groq Llama 3.1 (free) 2) OpenAI 3) Knowledge base fallback
 */

import { knowledgeItems, getAutomationPatterns, JOLIKA_MANAGERS, company } from '@/data/jolika-data'

// ========== GREETING & INTENT DETECTION ==========

// Common Hebrew greetings and their responses
const GREETINGS: Record<string, string> = {
  '×”×™×™': '×”×™×™! ××” ××¤×©×¨ ×œ×¢×–×•×¨?',
  '×©×œ×•×': '×©×œ×•×! ××™×š ××¤×©×¨ ×œ×¢×–×•×¨ ×œ×š?',
  '×”×™': '×”×™×™! ×‘××” ××•×›×œ ×œ×¡×™×™×¢?',
  '×‘×•×§×¨ ×˜×•×‘': '×‘×•×§×¨ ×˜×•×‘! ××™×š ××¤×©×¨ ×œ×¢×–×•×¨?',
  '×¢×¨×‘ ×˜×•×‘': '×¢×¨×‘ ×˜×•×‘! ××™×š ××¤×©×¨ ×œ×¢×–×•×¨?',
  '×¦×”×¨×™×™× ×˜×•×‘×™×': '×¦×”×¨×™×™× ×˜×•×‘×™×! ×‘××” ××•×›×œ ×œ×¡×™×™×¢?',
  '××” × ×©××¢': '×”×›×œ ×˜×•×‘! ××™×š ××¤×©×¨ ×œ×¢×–×•×¨?',
  '××” ×§×•×¨×”': '×”×›×œ ×˜×•×‘! ×‘××” ××•×›×œ ×œ×¢×–×•×¨?',
}

// Subject topics for guidance
const AVAILABLE_TOPICS = [
  { name: '××©×œ×•×—×™×', examples: ['××ª×™ ×™×© ××©×œ×•×—×™×?', '××™×–×•×¨×™ ××©×œ×•×—'] },
  { name: '×”×–×× ×•×ª', examples: ['××™×š ××§×‘×œ×™× ×”×–×× ×”?', '××™×š ×œ××¨×•×– ×”×–×× ×”?'] },
  { name: '××œ××™ ×•×¤×¨×œ×™× ×™×', examples: ['××” ×”×˜×¢××™× ×‘××œ××™?', '××™×œ×• ×¤×¨×œ×™× ×™× ×™×©?'] },
  { name: '×ª×©×œ×•××™×', examples: ['××™×š ××§×‘×œ×™× ×ª×©×œ×•×?', '×”×¢×‘×¨×” ×‘× ×§××™×ª'] },
  { name: '××•×¢×“×•×Ÿ ×œ×§×•×—×•×ª', examples: ['××” ×–×” ×•×œ×™×•×§××¨×“?', '×”× ×—×•×ª ×œ×¢×¡×§×™×'] },
  { name: '××œ×¨×’× ×™×', examples: ['××” ××›×™×œ ××’×•×–×™×?', '××œ×¨×’×™×” ×œ×’×œ×•×˜×Ÿ'] },
  { name: '× ×”×œ×™×', examples: ['×¤×ª×™×—×ª ××©××¨×ª', '×¡×’×™×¨×ª ×§×•×¤×”'] },
]

// Generate helpful guidance message
function getGuidanceMessage(): string {
  const topicsList = AVAILABLE_TOPICS.map(t => `â€¢ ${t.name}`).join('\n')
  return `×× ×™ ×™×›×•×œ ×œ×¢×–×•×¨ ×‘× ×•×©××™× ×”×‘××™×:
${topicsList}

×©××œ ×©××œ×” ×¡×¤×¦×™×¤×™×ª ×™×•×ª×¨, ×œ××©×œ:
"××ª×™ ×™×© ××©×œ×•×—×™× ×‘×™×•× ×©×™×©×™?" ××• "××” ×”×˜×¢××™× ×”×›×™ ×¤×•×¤×•×œ×¨×™×™×?"`
}

// Get topic suggestion based on partial query
function suggestTopics(query: string): string | null {
  const lower = query.toLowerCase()
  const suggestions: string[] = []

  // Check for partial matches
  if (lower.includes('××©×œ×•') || lower.includes('×©×œ×™×—')) {
    suggestions.push('××©×œ×•×—×™× - "××ª×™ ×™×© ××©×œ×•×—×™×?", "×œ××™×¤×” ××’×™×¢×™×?"')
  }
  if (lower.includes('×”×–×× ') || lower.includes('×œ×§×•×—')) {
    suggestions.push('×”×–×× ×•×ª - "××™×š ××§×‘×œ×™× ×”×–×× ×”?", "××™×š ×œ××¨×•×–?"')
  }
  if (lower.includes('××œ××™') || lower.includes('×¤×¨×œ×™× ') || lower.includes('×©×•×§×•×œ×“') || lower.includes('×˜×¢×')) {
    suggestions.push('××œ××™ - "××” ×”×˜×¢××™× ×”×¤×•×¤×•×œ×¨×™×™×?", "××” ×—×“×© ×‘××œ××™?"')
  }
  if (lower.includes('×ª×©×œ×•×') || lower.includes('×›×¡×£') || lower.includes('×”×¢×‘×¨')) {
    suggestions.push('×ª×©×œ×•××™× - "××™×š ××§×‘×œ×™× ×ª×©×œ×•×?", "×”×¢×‘×¨×” ×‘× ×§××™×ª"')
  }
  if (lower.includes('××•×¢×“×•×Ÿ') || lower.includes('×”× ×—') || lower.includes('×¢×¡×§')) {
    suggestions.push('××•×¢×“×•×Ÿ ×œ×§×•×—×•×ª - "××” ×–×” ×•×œ×™×•×§××¨×“?", "×”× ×—×•×ª ×œ×¢×¡×§×™×"')
  }
  if (lower.includes('××œ×¨×’') || lower.includes('××’×•×–') || lower.includes('×’×œ×•×˜×Ÿ')) {
    suggestions.push('××œ×¨×’× ×™× - "××” ××›×™×œ ××’×•×–×™×?", "×œ×œ× ×’×œ×•×˜×Ÿ"')
  }

  if (suggestions.length > 0) {
    return `×”×ª×›×•×•× ×ª ×œ×©××•×œ ×¢×œ:\n${suggestions.map(s => `â€¢ ${s}`).join('\n')}`
  }
  return null
}

// Manager info response
const MANAGER_INFO = `×”×× ×”×œ×•×ª ×©×œ ×’'×•×œ×™×§×” ×©×•×§×•×œ×“:
${JOLIKA_MANAGERS.map(m => `â€¢ ${m.name} - ${m.role}`).join('\n')}

×œ×©××œ×•×ª ×“×—×•×¤×•×ª ×¤× ×” ×œ×©×œ×™ ×’×•×œ×“× ×‘×¨×’ (×‘×¢×œ×™× ×•×× ×”×œ×ª ×¨××©×™×ª).`

// Check if query is a greeting
function isGreeting(query: string): string | null {
  const normalized = query.trim().replace(/[?.!,]/g, '')
  for (const [greeting, response] of Object.entries(GREETINGS)) {
    if (normalized === greeting || normalized.startsWith(greeting + ' ')) {
      return response
    }
  }
  return null
}

// Check if query is asking about managers
function isAskingAboutManager(query: string): boolean {
  const managerKeywords = ['×× ×”×œ', '×× ×”×œ×ª', '×‘×¢×œ×™×', '××—×¨××™', '××—×¨××™×ª', '××™ ×”××—×¨××™', '××™ ×”×× ×”×œ', '××™ ×”×‘×•×¡']
  const lower = query.toLowerCase()
  return managerKeywords.some(kw => lower.includes(kw))
}

// Check if query is unclear or too short
function isUnclearQuery(query: string): boolean {
  const normalized = query.trim().replace(/[?.!,]/g, '')
  // Very short queries that aren't greetings
  if (normalized.length <= 3) return true
  // Single characters or words like "××”" without context
  if (['××”', '×œ××”', '××™×š', '××ª×™', '××™×¤×”', '×›××”'].includes(normalized)) return true
  return false
}

// ========== TEXT NORMALIZATION ==========

// Text normalization for Hebrew
function normalizeHebrew(text: string): string {
  return text
    .toLowerCase()
    .replace(/[?.!,\-'"()×´×³]/g, '')
    .replace(/\s+/g, ' ')
    .trim()
}

// Hebrew word stemming (basic)
function stemHebrew(word: string): string {
  // Remove common Hebrew prefixes
  const prefixes = ['×”', '×•', '×‘', '×œ', '×', '×›', '×©']
  let result = word
  for (const prefix of prefixes) {
    if (result.startsWith(prefix) && result.length > 2) {
      result = result.slice(1)
      break
    }
  }
  // Remove common suffixes
  const suffixes = ['×™×', '×•×ª', '×”', '×ª']
  for (const suffix of suffixes) {
    if (result.endsWith(suffix) && result.length > 3) {
      result = result.slice(0, -suffix.length)
      break
    }
  }
  return result
}

// Extract meaningful keywords from query
function extractKeywords(text: string): string[] {
  const stopWords = ['××ª', '×©×œ', '×¢×œ', '×¢×', '××œ', '×–×”', '×–×•', '×–××ª', '×”×•×', '×”×™×', '×”×', '×”×Ÿ', '×× ×™', '××ª×”', '××ª', '×× ×—× ×•', '×œ×™', '×œ×š', '×œ×•', '×œ×”', '×›×Ÿ', '×œ×', '×’×', '×¨×§', '×›×œ', '×›××•', '××•', '××', '×›×™', '××‘×œ', '×¢×•×“', '×›×‘×¨', '×¤×”', '×©×', '××™×š', '××”', '××ª×™', '××™×¤×”', '×œ××”', '×›××”', '××™', '××–', '×™×©', '××™×Ÿ']

  const words = normalizeHebrew(text).split(' ')
  return words.filter(w => w.length > 1 && !stopWords.includes(w))
}

// ========== IMPROVED SIMILARITY CALCULATION ==========

// Calculate semantic similarity between query and content
function calculateSimilarity(query: string, content: string, exampleQuestions: string[] = []): number {
  const queryKeywords = extractKeywords(query)
  const contentNorm = normalizeHebrew(content)

  if (queryKeywords.length === 0) return 0

  let score = 0
  let matchedWords = 0
  let exactPhraseMatch = false

  // Check for exact phrase match in content or examples
  const queryNorm = normalizeHebrew(query)
  if (contentNorm.includes(queryNorm) && queryNorm.length > 3) {
    exactPhraseMatch = true
    score += 0.5
  }

  // Check example questions for strong matches
  for (const example of exampleQuestions) {
    const exampleNorm = normalizeHebrew(example)
    // Exact match with example question
    if (exampleNorm === queryNorm) {
      return 1.0 // Perfect match
    }
    // High similarity with example
    if (exampleNorm.includes(queryNorm) || queryNorm.includes(exampleNorm)) {
      score += 0.4
      break
    }
  }

  // Keyword matching
  for (const word of queryKeywords) {
    const stemmed = stemHebrew(word)

    // Exact word match
    if (contentNorm.includes(word)) {
      matchedWords++
      score += 0.15
    }
    // Stemmed match (partial credit)
    else if (contentNorm.includes(stemmed) && stemmed.length > 2) {
      matchedWords++
      score += 0.08
    }
  }

  // Calculate keyword coverage ratio
  const coverageRatio = matchedWords / queryKeywords.length

  // Penalize low coverage even if some words match
  if (coverageRatio < 0.3 && !exactPhraseMatch) {
    score *= 0.3 // Heavy penalty for low relevance
  }

  // Normalize score to 0-1 range
  return Math.min(score, 1.0)
}

// ========== KNOWLEDGE RETRIEVAL ==========

// Find relevant knowledge items with improved matching
export function findRelevantKnowledgeStatic(query: string, limit = 5) {
  const results: Array<{
    id: string
    title: string
    content: string
    type: string
    similarity: number
    isAutomation: boolean
    matchReason: string
  }> = []

  // Check automation patterns first (these have example questions)
  const patterns = getAutomationPatterns()
  for (let i = 0; i < patterns.length; i++) {
    const p = patterns[i]
    const similarity = calculateSimilarity(query, p.answer, p.exampleQuestions)

    // Also check title match
    const titleSim = calculateSimilarity(query, p.title)
    const finalSim = Math.max(similarity, titleSim * 0.8)

    // Only include if similarity is meaningful
    if (finalSim > 0.25) {
      results.push({
        id: `automation-${i}`,
        title: p.title,
        content: p.answer,
        type: 'automation',
        similarity: finalSim + 0.1, // Boost automation patterns
        isAutomation: true,
        matchReason: similarity > titleSim ? 'content' : 'title',
      })
    }
  }

  // Check knowledge items
  for (let i = 0; i < knowledgeItems.length; i++) {
    const item = knowledgeItems[i]
    const title = item.titleHe || item.title
    const content = item.contentHe || item.content
    const questions = item.example_questions || []

    const contentSim = calculateSimilarity(query, content, questions)
    const titleSim = calculateSimilarity(query, title)
    const finalSim = Math.max(contentSim, titleSim * 1.2)

    // Only include if similarity is meaningful
    if (finalSim > 0.2) {
      results.push({
        id: `kb-${i}`,
        title,
        content,
        type: item.type,
        similarity: finalSim,
        isAutomation: false,
        matchReason: contentSim > titleSim ? 'content' : 'title',
      })
    }
  }

  // Sort by similarity and return top results
  return results
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, limit)
}

// ========== LLM INTEGRATION ==========

// Try Ollama (local LLM - llama3.2 or any installed model) - Enhanced with interactive guidance
async function tryOllama(
  query: string,
  context: string,
  history: Array<{ role: string; content: string }>
): Promise<string | null> {
  const ollamaUrl = process.env.OLLAMA_URL || 'http://localhost:11434'
  const ollamaModel = process.env.OLLAMA_MODEL || 'llama3.2'

  try {
    // First check if Ollama is running
    const healthCheck = await fetch(`${ollamaUrl}/api/tags`, {
      signal: AbortSignal.timeout(2000), // 2 second timeout for health check
    }).catch(() => null)

    if (!healthCheck?.ok) {
      console.log('Ollama not available, skipping...')
      return null
    }

    // Determine if we have good context or need to guide the user
    const hasGoodContext = context && !context.includes('××™×Ÿ ××™×“×¢ ×¡×¤×¦×™×¤×™')

    const systemPrompt = hasGoodContext
      ? `××ª×” ×¢×•×–×¨ AI ×¤× ×™××™ ×œ×¢×•×‘×“×™ ${company.name} - ×—× ×•×ª ×©×•×§×•×œ×“ ×•××ª× ×•×ª ×‘×¨××ª ×”×©×¨×•×Ÿ.

×›×œ×œ×™× ×—×©×•×‘×™×:
1. ×¢× ×” ×‘×¢×‘×¨×™×ª ×ª××™×“
2. ×”×™×” ×ª××¦×™×ª×™ ×•×‘×¨×•×¨ - ×¢×“ 3 ××©×¤×˜×™×
3. ×”×©×ª××© ×‘××™×“×¢ ××”×××’×¨
4. ×ª×Ÿ ×ª×©×•×‘×” ××•×¢×™×œ×” ×’× ×× ×œ× ×‘×˜×•×— ×‘-100%
5. ×× ×”×©××œ×” ×œ× ×‘×¨×•×¨×” - ×”×¦×¢ ×©××œ×•×ª ×”××©×š

×××’×¨ ×™×“×¢:
${context}`
      : `××ª×” ×¢×•×–×¨ AI ×—×›× ×•×™×“×™×“×•×ª×™ ×œ×¢×•×‘×“×™ ${company.name} - ×—× ×•×ª ×©×•×§×•×œ×“ ×•××ª× ×•×ª ×‘×¨××ª ×”×©×¨×•×Ÿ.

××™×Ÿ ×œ×š ××™×“×¢ ×¡×¤×¦×™×¤×™ ×¢×œ ×”×©××œ×” ×”×–×•, ××‘×œ ×¢×œ×™×š ×œ×¢×–×•×¨ ×‘×¦×•×¨×” ××™× ×˜×¨××§×˜×™×‘×™×ª:

1. ×¢× ×” ×‘×¢×‘×¨×™×ª ×ª××™×“
2. ×”×›×¨ ×‘×›×š ×©××™×Ÿ ×œ×š ××ª ×”××™×“×¢ ×”××“×•×™×§
3. ×”×¦×¢ 2-3 ×©××œ×•×ª ×××•×§×“×•×ª ×©×™×¢×–×¨×• ×œ×š ×œ×”×‘×™×Ÿ ××” ×”××©×ª××© ×¦×¨×™×š
4. ×”×¦×¢ ×œ×¤× ×•×ª ×œ×©×œ×™ (×”×× ×”×œ×ª) ×× ×–×” ×“×—×•×£
5. ×ª×Ÿ ×”×¨×’×©×” ×˜×•×‘×” - ××ª×” ×›××Ÿ ×œ×¢×–×•×¨!

×“×•×’××” ×œ×ª×©×•×‘×” ×˜×•×‘×”:
"×× ×™ ×œ× ××¦××ª×™ ××ª ×”××™×“×¢ ×”×¡×¤×¦×™×¤×™ ×”×–×” ×‘×××’×¨, ××‘×œ ××©××— ×œ×¢×–×•×¨!
â€¢ ×”×× ××ª×” ××—×¤×© ××™×“×¢ ×¢×œ [××¤×©×¨×•×ª 1]?
â€¢ ××• ××•×œ×™ ×¢×œ [××¤×©×¨×•×ª 2]?
â€¢ ×× ×–×” ×“×—×•×£, ×©×œ×™ ×ª××™×“ ×©××—×” ×œ×¢×–×•×¨ ×™×©×™×¨×•×ª."

× ×•×©××™× ×©×× ×™ ×™×•×“×¢ ×¢×œ×™×”×: ××©×œ×•×—×™×, ×”×–×× ×•×ª, ××œ××™ ×•×¤×¨×œ×™× ×™×, ×ª×©×œ×•××™×, ××•×¢×“×•×Ÿ ×œ×§×•×—×•×ª, ××œ×¨×’× ×™×, × ×”×œ×™×.`

    // Build messages for Ollama chat format
    const messages = [
      { role: 'system', content: systemPrompt },
      ...history.slice(-6),
      { role: 'user', content: query },
    ]

    const res = await fetch(`${ollamaUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: ollamaModel,
        messages,
        stream: false,
        options: {
          temperature: hasGoodContext ? 0.3 : 0.6, // Higher creativity when guiding
          num_predict: 500,
        },
      }),
      signal: AbortSignal.timeout(30000), // 30 second timeout for response
    })

    if (!res.ok) {
      console.error('Ollama error:', res.status)
      return null
    }

    const data = await res.json()
    return data.message?.content || null
  } catch (e) {
    console.error('Ollama error:', e)
    return null
  }
}

// Try Groq API (free Llama 3.1) - Enhanced with interactive guidance
async function tryGroq(
  query: string,
  context: string,
  history: Array<{ role: string; content: string }>
): Promise<string | null> {
  const apiKey = process.env.GROQ_API_KEY
  if (!apiKey) return null

  try {
    // Determine if we have good context or need to guide the user
    const hasGoodContext = context && !context.includes('××™×Ÿ ××™×“×¢ ×¡×¤×¦×™×¤×™')

    const systemPrompt = hasGoodContext
      ? `××ª×” ×¢×•×–×¨ AI ×¤× ×™××™ ×œ×¢×•×‘×“×™ ${company.name} - ×—× ×•×ª ×©×•×§×•×œ×“ ×•××ª× ×•×ª ×‘×¨××ª ×”×©×¨×•×Ÿ.

×›×œ×œ×™× ×—×©×•×‘×™×:
1. ×¢× ×” ×‘×¢×‘×¨×™×ª ×ª××™×“
2. ×”×™×” ×ª××¦×™×ª×™ ×•×‘×¨×•×¨ - ×¢×“ 3 ××©×¤×˜×™×
3. ×”×©×ª××© ×‘××™×“×¢ ××”×××’×¨
4. ×ª×Ÿ ×ª×©×•×‘×” ××•×¢×™×œ×” ×’× ×× ×œ× ×‘×˜×•×— ×‘-100%
5. ×× ×”×©××œ×” ×œ× ×‘×¨×•×¨×” - ×”×¦×¢ ×©××œ×•×ª ×”××©×š

×××’×¨ ×™×“×¢:
${context}`
      : `××ª×” ×¢×•×–×¨ AI ×—×›× ×•×™×“×™×“×•×ª×™ ×œ×¢×•×‘×“×™ ${company.name} - ×—× ×•×ª ×©×•×§×•×œ×“ ×•××ª× ×•×ª ×‘×¨××ª ×”×©×¨×•×Ÿ.

××™×Ÿ ×œ×š ××™×“×¢ ×¡×¤×¦×™×¤×™ ×¢×œ ×”×©××œ×” ×”×–×•, ××‘×œ ×¢×œ×™×š ×œ×¢×–×•×¨ ×‘×¦×•×¨×” ××™× ×˜×¨××§×˜×™×‘×™×ª:

1. ×¢× ×” ×‘×¢×‘×¨×™×ª ×ª××™×“
2. ×”×›×¨ ×‘×›×š ×©××™×Ÿ ×œ×š ××ª ×”××™×“×¢ ×”××“×•×™×§
3. ×”×¦×¢ 2-3 ×©××œ×•×ª ×××•×§×“×•×ª ×©×™×¢×–×¨×• ×œ×š ×œ×”×‘×™×Ÿ ××” ×”××©×ª××© ×¦×¨×™×š
4. ×”×¦×¢ ×œ×¤× ×•×ª ×œ×©×œ×™ (×”×× ×”×œ×ª) ×× ×–×” ×“×—×•×£
5. ×ª×Ÿ ×”×¨×’×©×” ×˜×•×‘×” - ××ª×” ×›××Ÿ ×œ×¢×–×•×¨!

×“×•×’××” ×œ×ª×©×•×‘×” ×˜×•×‘×”:
"×× ×™ ×œ× ××¦××ª×™ ××ª ×”××™×“×¢ ×”×¡×¤×¦×™×¤×™ ×”×–×” ×‘×××’×¨, ××‘×œ ××©××— ×œ×¢×–×•×¨!
â€¢ ×”×× ××ª×” ××—×¤×© ××™×“×¢ ×¢×œ [××¤×©×¨×•×ª 1]?
â€¢ ××• ××•×œ×™ ×¢×œ [××¤×©×¨×•×ª 2]?
â€¢ ×× ×–×” ×“×—×•×£, ×©×œ×™ ×ª××™×“ ×©××—×” ×œ×¢×–×•×¨ ×™×©×™×¨×•×ª."

× ×•×©××™× ×©×× ×™ ×™×•×“×¢ ×¢×œ×™×”×: ××©×œ×•×—×™×, ×”×–×× ×•×ª, ××œ××™ ×•×¤×¨×œ×™× ×™×, ×ª×©×œ×•××™×, ××•×¢×“×•×Ÿ ×œ×§×•×—×•×ª, ××œ×¨×’× ×™×, × ×”×œ×™×.`

    const messages = [
      { role: 'system', content: systemPrompt },
      ...history.slice(-6),
      { role: 'user', content: query },
    ]

    const res = await fetch('https://api.groq.com/openai/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'llama-3.1-8b-instant',
        messages,
        temperature: hasGoodContext ? 0.3 : 0.6, // Higher creativity when guiding
        max_tokens: 500,
      }),
    })

    if (!res.ok) {
      console.error('Groq error:', res.status)
      return null
    }

    const data = await res.json()
    return data.choices?.[0]?.message?.content || null
  } catch (e) {
    console.error('Groq error:', e)
    return null
  }
}

// Try OpenAI API - Enhanced with interactive guidance
async function tryOpenAI(
  query: string,
  context: string,
  history: Array<{ role: string; content: string }>
): Promise<string | null> {
  const apiKey = process.env.OPENAI_API_KEY
  if (!apiKey) return null

  try {
    // Determine if we have good context or need to guide the user
    const hasGoodContext = context && !context.includes('××™×Ÿ ××™×“×¢ ×¡×¤×¦×™×¤×™')

    const systemPrompt = hasGoodContext
      ? `××ª×” ×¢×•×–×¨ AI ×¤× ×™××™ ×œ×¢×•×‘×“×™ ${company.name} - ×—× ×•×ª ×©×•×§×•×œ×“ ×•××ª× ×•×ª.

×›×œ×œ×™×:
1. ×¢× ×” ×‘×¢×‘×¨×™×ª ×‘×œ×‘×“
2. ×ª×©×•×‘×•×ª ×§×¦×¨×•×ª ×•×××•×§×“×•×ª
3. ×”×©×ª××© ×‘××™×“×¢ ××”×××’×¨
4. ×ª×Ÿ ×ª×©×•×‘×” ××•×¢×™×œ×” ×’× ×× ×œ× ×‘×˜×•×— ×‘-100%

×××’×¨ ×™×“×¢:
${context}`
      : `××ª×” ×¢×•×–×¨ AI ×—×›× ×•×™×“×™×“×•×ª×™ ×œ×¢×•×‘×“×™ ${company.name} - ×—× ×•×ª ×©×•×§×•×œ×“ ×•××ª× ×•×ª.

××™×Ÿ ×œ×š ××™×“×¢ ×¡×¤×¦×™×¤×™ ×¢×œ ×”×©××œ×” ×”×–×•, ××‘×œ ×¢×œ×™×š ×œ×¢×–×•×¨ ×‘×¦×•×¨×” ××™× ×˜×¨××§×˜×™×‘×™×ª:

1. ×¢× ×” ×‘×¢×‘×¨×™×ª ×‘×œ×‘×“
2. ×”×›×¨ ×©××™×Ÿ ×œ×š ×”××™×“×¢ ×”××“×•×™×§
3. ×”×¦×¢ 2-3 ×©××œ×•×ª ×××•×§×“×•×ª ×©×™×¢×–×¨×• ×œ×”×‘×™×Ÿ ××” ×”××©×ª××© ×¦×¨×™×š
4. ×”×¦×¢ ×œ×¤× ×•×ª ×œ×©×œ×™ (×”×× ×”×œ×ª) ×× ×“×—×•×£
5. ×ª×Ÿ ×”×¨×’×©×” ×˜×•×‘×” - ××ª×” ×›××Ÿ ×œ×¢×–×•×¨!

× ×•×©××™× ×©××ª×” ×™×•×“×¢ ×¢×œ×™×”×: ××©×œ×•×—×™×, ×”×–×× ×•×ª, ××œ××™ ×•×¤×¨×œ×™× ×™×, ×ª×©×œ×•××™×, ××•×¢×“×•×Ÿ ×œ×§×•×—×•×ª, ××œ×¨×’× ×™×, × ×”×œ×™×.`

    const messages = [
      { role: 'system', content: systemPrompt },
      ...history.slice(-6),
      { role: 'user', content: query },
    ]

    const res = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages,
        temperature: hasGoodContext ? 0.3 : 0.6, // Higher creativity when guiding
        max_tokens: 500,
      }),
    })

    if (!res.ok) {
      console.error('OpenAI error:', res.status)
      return null
    }

    const data = await res.json()
    return data.choices?.[0]?.message?.content || null
  } catch (e) {
    console.error('OpenAI error:', e)
    return null
  }
}

// ========== MAIN RESPONSE GENERATION ==========

export async function generateResponseStatic(
  query: string,
  conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }> = []
): Promise<{
  response: string
  responseHe: string
  knowledgeItemId: string | null
  confidence: number
  mediaUrls: string[]
  isAutomatedResponse: boolean
}> {
  // 1. Check for greetings first
  const greetingResponse = isGreeting(query)
  if (greetingResponse) {
    return {
      response: greetingResponse,
      responseHe: greetingResponse,
      knowledgeItemId: null,
      confidence: 1.0,
      mediaUrls: [],
      isAutomatedResponse: true,
    }
  }

  // 2. Check for unclear/too short queries - provide guidance
  if (isUnclearQuery(query)) {
    const response = getGuidanceMessage()
    return {
      response,
      responseHe: response,
      knowledgeItemId: null,
      confidence: 0,
      mediaUrls: [],
      isAutomatedResponse: true,
    }
  }

  // 3. Check for manager-related questions
  if (isAskingAboutManager(query)) {
    return {
      response: MANAGER_INFO,
      responseHe: MANAGER_INFO,
      knowledgeItemId: null,
      confidence: 1.0,
      mediaUrls: [],
      isAutomatedResponse: true,
    }
  }

  // 4. Find relevant knowledge
  const relevantKnowledge = findRelevantKnowledgeStatic(query, 5)
  const topMatch = relevantKnowledge[0]

  // 5. High-confidence direct match - return automation answer
  if (topMatch?.isAutomation && topMatch.similarity > 0.6) {
    return {
      response: topMatch.content,
      responseHe: topMatch.content,
      knowledgeItemId: topMatch.id,
      confidence: topMatch.similarity,
      mediaUrls: [],
      isAutomatedResponse: true,
    }
  }

  // 6. Build context from relevant knowledge (only high-quality matches)
  const qualityMatches = relevantKnowledge.filter(k => k.similarity > 0.3)
  const context = qualityMatches.length > 0
    ? qualityMatches.map(k => `ğŸ“Œ ${k.title}:\n${k.content}`).join('\n\n')
    : '××™×Ÿ ××™×“×¢ ×¡×¤×¦×™×¤×™ ×‘×××’×¨ ×¢×œ ×©××œ×” ×–×•.'

  const confidence = qualityMatches.length > 0 ? topMatch?.similarity || 0 : 0
  const history = conversationHistory.map(m => ({ role: m.role, content: m.content }))

  // 7. Try LLM providers in order: Groq (free) â†’ OpenAI â†’ Ollama (local)
  let llmResponse = await tryGroq(query, context, history)
  if (!llmResponse) {
    llmResponse = await tryOpenAI(query, context, history)
  }
  if (!llmResponse) {
    llmResponse = await tryOllama(query, context, history)
  }

  if (llmResponse) {
    return {
      response: llmResponse,
      responseHe: llmResponse,
      knowledgeItemId: topMatch?.id || null,
      confidence,
      mediaUrls: [],
      isAutomatedResponse: false,
    }
  }

  // 8. Fallback to knowledge base only if we have a good match
  if (qualityMatches.length > 0 && topMatch && topMatch.similarity > 0.4) {
    const response = topMatch.content.length > 400
      ? topMatch.content.slice(0, 400) + '...'
      : topMatch.content

    return {
      response,
      responseHe: response,
      knowledgeItemId: topMatch.id,
      confidence,
      mediaUrls: [],
      isAutomatedResponse: false,
    }
  }

  // 9. No good match found - try to provide a helpful response anyway (Grok style)
  // Instead of just saying "I don't know", provide best-effort answer with disclaimer

  // Build a helpful response based on what we know about the business
  const query_lower = query.toLowerCase()
  let helpfulResponse = ''

  // Try to infer intent and provide helpful guidance
  if (query_lower.includes('××—×™×¨') || query_lower.includes('×¢×œ×•×ª') || query_lower.includes('×›××” ×¢×•×œ×”')) {
    helpfulResponse = `×œ×’×‘×™ ××—×™×¨×™× - ×× ×™ ×œ× ×‘×˜×•×— ×‘××—×™×¨ ×”××“×•×™×§, ××‘×œ ××ª×” ××•×–××Ÿ ×œ×‘×“×•×§ ×‘××ª×¨ ××• ×œ×©××•×œ ××ª ×©×œ×™ ×™×©×™×¨×•×ª. ×”×™× ×ª×•×›×œ ×œ×ª×ª ×œ×š ××ª ×”××—×™×¨ ×”××¢×•×“×›×Ÿ.`
  } else if (query_lower.includes('×©×¢×•×ª') || query_lower.includes('×¤×ª×•×—') || query_lower.includes('×¡×’×•×¨')) {
    helpfulResponse = `×œ×’×‘×™ ×©×¢×•×ª ×¤×¢×™×œ×•×ª - ×›×“××™ ×œ×‘×“×•×§ ××•×œ ×©×œ×™ ××• ×‘××ª×¨ ×œ×©×¢×•×ª ×”××¢×•×“×›× ×•×ª. ×‘×“×¨×š ×›×œ×œ ×”×—× ×•×ª ×¤×ª×•×—×” ×‘×©×¢×•×ª ×”×¢×‘×•×“×” ×”×¨×’×™×œ×•×ª.`
  } else if (query_lower.includes('××©×œ×•×—') || query_lower.includes('×©×œ×™×—')) {
    helpfulResponse = `×œ×’×‘×™ ××©×œ×•×—×™× - ×™×© ××©×œ×•×—×™× ×‘××–×•×¨ ×¨××ª ×”×©×¨×•×Ÿ ×•×”×¡×‘×™×‘×”. ×œ×¤×¨×˜×™× ××“×•×™×§×™× ×¢×œ ××–×•×¨×™ ×”××©×œ×•×— ×•×”××—×™×¨×™×, ×›×“××™ ×œ×‘×“×•×§ ×¢× ×©×œ×™.`
  } else if (query_lower.includes('×”×–×× ×”') || query_lower.includes('×œ×”×–××™×Ÿ')) {
    helpfulResponse = `×œ×’×‘×™ ×”×–×× ×•×ª - ××¤×©×¨ ×œ×”×–××™×Ÿ ×“×¨×š ×”×—× ×•×ª, ×‘××ª×¨, ××• ×‘×˜×œ×¤×•×Ÿ. ×œ×¤×¨×˜×™× × ×•×¡×¤×™× ××• ×”×–×× ×•×ª ××™×•×—×“×•×ª, ×¤× ×” ×œ×©×œ×™.`
  } else {
    // Generic helpful response
    helpfulResponse = `×× ×™ ×œ× ××¦××ª×™ ×ª×©×•×‘×” ××“×•×™×§×ª ×œ×©××œ×” ×”×–×• ×‘×××’×¨ ×”×™×“×¢ ×©×œ×™.

××‘×œ ××œ ×“××’×”! ×”× ×” ××” ×©×× ×™ ×™×›×•×œ ×œ×”×¦×™×¢:
â€¢ × ×¡×” ×œ× ×¡×— ××ª ×”×©××œ×” ×‘×¦×•×¨×” ×§×¦×ª ×©×•× ×”
â€¢ ×©××œ ××ª ×©×œ×™ ×™×©×™×¨×•×ª - ×”×™× ×ª××™×“ ×©××—×” ×œ×¢×–×•×¨
â€¢ ×× ×–×” ×“×—×•×£, ×”×ª×§×©×¨ ×œ×—× ×•×ª

××™×š ×¢×•×“ ××•×›×œ ×œ×¢×–×•×¨?`
  }

  const topicSuggestion = suggestTopics(query)
  if (topicSuggestion) {
    helpfulResponse += `\n\n${topicSuggestion}`
  }

  return {
    response: helpfulResponse,
    responseHe: helpfulResponse,
    knowledgeItemId: null,
    confidence: 0.3, // Low but not zero - we're still trying to help
    mediaUrls: [],
    isAutomatedResponse: true,
  }
}
